# NOTES: this Kinetics loader code has been modified to better aligned with the dataset export format
# generated by https://github.com/cvdfoundation/kinetics-dataset as of 6/1/2023. This requires changing
# the parsing of the annotation file and the structure of the folders.

# coding=utf-8
import os
import csv
import math
import cv2
from tqdm import tqdm
import pickle
import numpy as np
import torch
import torch.nn.functional as F
import torch.distributed as dist
from torchvision.io import read_video

import utils.logging as logging
from datasets.data_augment import create_data_augment, create_ssl_data_augment

from utils.decord_loader import decord_load


logger = logging.get_logger(__name__)

class K400(torch.utils.data.Dataset):
    def __init__(self, cfg, local_rank=0):
        self.cfg = cfg
        self.num_contexts = cfg.DATA.NUM_CONTEXTS
        # self.train_dataset = os.path.join(cfg.args.workdir, f"Kinetics400/train.csv")
        # self.train_dataset = os.path.join(cfg.args.workdir, "kinetics_400/k400/annotations/train.csv")
        self.train_dataset = "/datasets01/kinetics_400/k400/annotations/train.csv"

        self.local_rank = local_rank

        with open(self.train_dataset, 'r') as f:
            reader = csv.reader(f)
            # dataset = [{"video_file": row[0], "video_label": row[1]} for row in reader]
            dataset = []
            for r, row in enumerate(reader):
                if r == 0: continue
                e = {
                    "video_id": row[1],
                    "time_start": row[2],
                    "time_end": row[3],
                    "video_file": "%s_%06i_%06i.mp4"%(row[1], int(row[2]), int(row[3])),
                }
                dataset.append(e)

        self.dataset = []
        
        # CHANGE: load list of missing / error videos
        self.error_videos = set()
        if os.path.isfile("k400_missing.txt"):
            with open("k400_missing.txt", 'r') as f:
                for line in f:
                    self.error_videos.add(line.strip())
        if os.path.isfile("k400_error_files.txt")
            with open("k400_error_files.txt", 'r') as f:
                for line in f:
                    self.error_videos.add(line.strip())
        print('%i known error videos'%len(self.error_videos))

        for data in dataset:
            if data["video_file"] not in self.error_videos:
                self.dataset.append(data)

        logger.info(f"{len(self.dataset)} samples of Kinetics400 dataset have been read.")

        # Perform data-augmentation
        self.num_frames = cfg.TRAIN.NUM_FRAMES
        # CHANGE: for training, preproc has been moved to run GPU-side for efficiency
        # self.data_preprocess = create_ssl_data_augment(cfg, augment=True)
        self.data_preprocess = None

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, index):        
        # name = self.dataset[index]["video_file"].split("/")[-1]
        name = self.dataset[index]["video_file"]

        # video_file = os.path.join(self.cfg.args.workdir, 'kinetics_400/k400/train', self.dataset[index]["video_file"])
        video_file = os.path.join('/datasets01/kinetics_400/k400/train', self.dataset[index]["video_file"])
        
        # get frame count
        data = cv2.VideoCapture(video_file)
        seq_len = int(data.get(cv2.CAP_PROP_FRAME_COUNT))
        frame_label = -1 * torch.ones(seq_len)

        # CHANGE: extra handling for empty/corrupted videos
        if seq_len == 0:
            print('ERROR VIDEO:')
            print(video_file)
            with open("k400_error_files.txt", 'a') as f:
                f.write(self.dataset[index]["video_file"] + '\n')
                print('WARNING: Corrupted file: ' + self.dataset[index]["video_file"])
            # return first video as placeholder
            return self.__getitem__(0)

        # CHANGE: fast data loading with Decord
        steps_0, chosen_step_0, video_mask0 = self.sample_frames(seq_len, self.num_frames)
        steps_1, chosen_step_1, video_mask1 = self.sample_frames(seq_len, self.num_frames, pre_steps=steps_0)
        s_start = min(int(steps_0[0]), int(steps_1[0]))
        s_stop = max(int(steps_0[-1]), int(steps_1[-1]))
        video = decord_load(video_file, s_start, s_stop+1)
        steps_0 -= s_start
        steps_1 -= s_start
        view_0 = video[steps_0]
        view_1 = video[steps_1]
        view_0 = view_0.permute(0,3,1,2).float() / 255.0 # T C H W, [0,1] tensor
        view_1 = view_1.permute(0,3,1,2).float() / 255.0 # T C H W, [0,1] tensor
        videos = (view_0, view_1)
        
        # CHANGE: preproc moved to GPU-side for efficiency
        names = [name, name]
        # steps_0, chosen_step_0, video_mask0 = self.sample_frames(seq_len, self.num_frames)
        # view_0 = self.data_preprocess(video[steps_0.long()])
        # view_0 = video[steps_0.long()]
        label_0 = frame_label[chosen_step_0.long()]
        # steps_1, chosen_step_1, video_mask1 = self.sample_frames(seq_len, self.num_frames, pre_steps=steps_0)
        # view_1 = self.data_preprocess(video[steps_1.long()])
        # view_1 = video[steps_1.long()]
        label_1 = frame_label[chosen_step_1.long()]
        # videos = torch.stack([view_0, view_1], dim=0)
        # videos = (view_0, view_1)
        labels = torch.stack([label_0, label_1], dim=0)
        seq_lens = torch.tensor([seq_len, seq_len])
        chosen_steps = torch.stack([chosen_step_0, chosen_step_1], dim=0)
        video_mask = torch.stack([video_mask0, video_mask1], dim=0)
        return videos, labels, seq_lens, chosen_steps, video_mask, names

    def sample_frames(self, seq_len, num_frames, pre_steps=None):
        # When dealing with very long videos we can choose to sub-sample to fit
        # data in memory. But be aware this also evaluates over a subset of frames.
        # Subsampling the validation set videos when reporting performance is not
        # recommended.
        sampling_strategy = self.cfg.DATA.SAMPLING_STRATEGY
        pre_offset = min(pre_steps) if pre_steps is not None else None
        
        if sampling_strategy == 'offset_uniform':
            # Sample a random offset less than a provided max offset. Among all frames
            # higher than the chosen offset, randomly sample num_frames
            if seq_len >= num_frames:
                steps = torch.randperm(seq_len) # Returns a random permutation of integers from 0 to n - 1.
                steps = torch.sort(steps[:num_frames])[0]
            else:
                steps = torch.arange(0, num_frames)
        elif sampling_strategy == 'time_augment':
            num_valid = min(seq_len, num_frames)
            expand_ratio = np.random.uniform(low=1.0, high=self.cfg.DATA.SAMPLING_REGION) if self.cfg.DATA.SAMPLING_REGION>1 else 1.0

            block_size = math.ceil(expand_ratio*seq_len)
            if pre_steps is not None and self.cfg.DATA.CONSISTENT_OFFSET != 0:
                shift = int((1-self.cfg.DATA.CONSISTENT_OFFSET)*num_valid)
                offset = np.random.randint(low=max(0, min(seq_len-block_size, pre_offset-shift)), high=max(1, min(seq_len-block_size+1, pre_offset+shift+1)))
            else:
                offset = np.random.randint(low=0, high=max(seq_len-block_size, 1))
            steps = offset + torch.randperm(block_size)[:num_valid]
            steps = torch.sort(steps)[0]
            if num_valid < num_frames:
                steps = F.pad(steps, (0, num_frames-num_valid), "constant", seq_len)
        else:
            raise ValueError('Sampling strategy %s is unknown. Supported values are '
                            'stride, offset_uniform .' % sampling_strategy)

        video_mask = torch.ones(num_frames)
        video_mask[steps<0] = 0
        video_mask[steps>=seq_len] = 0
        # Store chosen indices.
        chosen_steps = torch.clamp(steps.clone(), 0, seq_len - 1)
        if self.num_contexts == 1:
            steps = chosen_steps
        else:
            # Get multiple context steps depending on config at selected steps.
            context_stride = self.cfg.DATA.CONTEXT_STRIDE
            steps = steps.view(-1,1) + context_stride*torch.arange(-(self.num_contexts-1), 1).view(1,-1)
            steps = torch.clamp(steps.view(-1), 0, seq_len - 1)

        return steps, chosen_steps, video_mask
